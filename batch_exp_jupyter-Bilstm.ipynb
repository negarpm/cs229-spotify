{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "import csv\n",
    "\n",
    "EPOCH_LOSSES_SAVEPATH = \"./models/bilstm_epoch_losses.csv\"\n",
    "BATCH_LOSSES_SAVEPATH = \"./models/bilstm_batch_losses.csv\"\n",
    "\n",
    "def save_list_to_csv(lst, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(lst)\n",
    "\n",
    "def evaluate_batch_mean_average_accuracy(y_truth, y_pred):\n",
    "    matches = [x == y for (x,y) in zip(y_truth, y_pred)]\n",
    "    maas = []\n",
    "    for batch in range(0, len(matches), 10):\n",
    "        num_correct = 0\n",
    "        summ = 0\n",
    "        for i in range(0, 10):\n",
    "            if matches[batch+i] == 1:\n",
    "                num_correct += 1\n",
    "                summ += (num_correct / (i+1)) / 10\n",
    "        maas.append(summ)\n",
    "    return sum(maas) / len(maas)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from models.BiLSTMEncoderDecoder import BiLSTMEncoderDecoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./models/bilstm_encoder_decoder_1.pt\"\n",
    "\n",
    "from data.SpotifyDataset import SpotifyDataset\n",
    "\n",
    "train_set = SpotifyDataset(\"./data/train_data_20.csv\", \"./data/track_feats.csv\")\n",
    "val_set = SpotifyDataset(\"./data/val_data_20.csv\", \"./data/track_feats.csv\")\n",
    "test_set = SpotifyDataset(\"./data/test_data_20.csv\", \"./data/track_feats.csv\")\n",
    "\n",
    "datasets = {\"train\": train_set,\n",
    "            \"val\": val_set,\n",
    "            \"test\": test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "\n",
    "def default_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return default_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Running train phase...\n",
      "Calculating batch 0 / 924\n",
      "Calculating batch 10 / 924\n",
      "Calculating batch 20 / 924\n",
      "Calculating batch 30 / 924\n",
      "Calculating batch 40 / 924\n",
      "Calculating batch 50 / 924\n",
      "Calculating batch 60 / 924\n",
      "Calculating batch 70 / 924\n",
      "Calculating batch 80 / 924\n",
      "Calculating batch 90 / 924\n",
      "Calculating batch 100 / 924\n",
      "Calculating batch 110 / 924\n",
      "Calculating batch 120 / 924\n",
      "Calculating batch 130 / 924\n",
      "Calculating batch 140 / 924\n",
      "Calculating batch 150 / 924\n",
      "Calculating batch 160 / 924\n",
      "Calculating batch 170 / 924\n",
      "Calculating batch 180 / 924\n",
      "Calculating batch 190 / 924\n",
      "Calculating batch 200 / 924\n",
      "Calculating batch 210 / 924\n",
      "Calculating batch 220 / 924\n",
      "Calculating batch 230 / 924\n",
      "Calculating batch 240 / 924\n",
      "Calculating batch 250 / 924\n",
      "Calculating batch 260 / 924\n",
      "Calculating batch 270 / 924\n",
      "Calculating batch 280 / 924\n",
      "Calculating batch 290 / 924\n",
      "Calculating batch 300 / 924\n",
      "Calculating batch 310 / 924\n",
      "Calculating batch 320 / 924\n",
      "Calculating batch 330 / 924\n",
      "Calculating batch 340 / 924\n",
      "Calculating batch 350 / 924\n",
      "Calculating batch 360 / 924\n",
      "Calculating batch 370 / 924\n",
      "Calculating batch 380 / 924\n",
      "Calculating batch 390 / 924\n",
      "Calculating batch 400 / 924\n",
      "Epoch 0 Avg. Loss: 0.6175587773323059\n",
      "Running val phase...\n",
      "Calculating batch 0 / 59\n",
      "Calculating batch 10 / 59\n",
      "Calculating batch 20 / 59\n",
      "Calculating batch 30 / 59\n",
      "Calculating batch 40 / 59\n",
      "Calculating batch 50 / 59\n",
      "Epoch 0 Avg. MAA: 0.5775702837560621, Best MAA: 0\n",
      "\n",
      "Epoch 2/3\n",
      "Running train phase...\n",
      "Calculating batch 0 / 924\n",
      "Calculating batch 10 / 924\n",
      "Calculating batch 20 / 924\n",
      "Calculating batch 30 / 924\n",
      "Calculating batch 40 / 924\n",
      "Calculating batch 50 / 924\n",
      "Calculating batch 60 / 924\n",
      "Calculating batch 70 / 924\n",
      "Calculating batch 80 / 924\n",
      "Calculating batch 90 / 924\n",
      "Calculating batch 100 / 924\n",
      "Calculating batch 110 / 924\n",
      "Calculating batch 120 / 924\n",
      "Calculating batch 130 / 924\n",
      "Calculating batch 140 / 924\n",
      "Calculating batch 150 / 924\n",
      "Calculating batch 160 / 924\n",
      "Calculating batch 170 / 924\n",
      "Calculating batch 180 / 924\n",
      "Calculating batch 190 / 924\n",
      "Calculating batch 200 / 924\n",
      "Calculating batch 210 / 924\n",
      "Calculating batch 220 / 924\n",
      "Calculating batch 230 / 924\n",
      "Calculating batch 240 / 924\n",
      "Calculating batch 250 / 924\n",
      "Calculating batch 260 / 924\n",
      "Calculating batch 270 / 924\n",
      "Calculating batch 280 / 924\n",
      "Calculating batch 290 / 924\n",
      "Calculating batch 300 / 924\n",
      "Calculating batch 310 / 924\n",
      "Calculating batch 320 / 924\n",
      "Calculating batch 330 / 924\n",
      "Calculating batch 340 / 924\n",
      "Calculating batch 350 / 924\n",
      "Calculating batch 360 / 924\n",
      "Calculating batch 370 / 924\n",
      "Calculating batch 380 / 924\n",
      "Calculating batch 390 / 924\n",
      "Calculating batch 400 / 924\n",
      "Epoch 1 Avg. Loss: 0.6089193820953369\n",
      "Running val phase...\n",
      "Calculating batch 0 / 59\n",
      "Calculating batch 10 / 59\n",
      "Calculating batch 20 / 59\n",
      "Calculating batch 30 / 59\n",
      "Calculating batch 40 / 59\n",
      "Calculating batch 50 / 59\n",
      "Epoch 1 Avg. MAA: 0.5789808541516696, Best MAA: 0.5775702837560621\n",
      "\n",
      "Epoch 3/3\n",
      "Running train phase...\n",
      "Calculating batch 0 / 924\n",
      "Calculating batch 10 / 924\n",
      "Calculating batch 20 / 924\n",
      "Calculating batch 30 / 924\n",
      "Calculating batch 40 / 924\n",
      "Calculating batch 50 / 924\n",
      "Calculating batch 60 / 924\n",
      "Calculating batch 70 / 924\n",
      "Calculating batch 80 / 924\n",
      "Calculating batch 90 / 924\n",
      "Calculating batch 100 / 924\n",
      "Calculating batch 110 / 924\n",
      "Calculating batch 120 / 924\n",
      "Calculating batch 130 / 924\n",
      "Calculating batch 140 / 924\n",
      "Calculating batch 150 / 924\n",
      "Calculating batch 160 / 924\n",
      "Calculating batch 170 / 924\n",
      "Calculating batch 180 / 924\n",
      "Calculating batch 190 / 924\n",
      "Calculating batch 200 / 924\n",
      "Calculating batch 210 / 924\n",
      "Calculating batch 220 / 924\n",
      "Calculating batch 230 / 924\n",
      "Calculating batch 240 / 924\n",
      "Calculating batch 250 / 924\n",
      "Calculating batch 260 / 924\n",
      "Calculating batch 270 / 924\n",
      "Calculating batch 280 / 924\n",
      "Calculating batch 290 / 924\n",
      "Calculating batch 300 / 924\n",
      "Calculating batch 310 / 924\n",
      "Calculating batch 320 / 924\n",
      "Calculating batch 330 / 924\n",
      "Calculating batch 340 / 924\n",
      "Calculating batch 350 / 924\n",
      "Calculating batch 360 / 924\n",
      "Calculating batch 370 / 924\n",
      "Calculating batch 380 / 924\n",
      "Calculating batch 390 / 924\n",
      "Calculating batch 400 / 924\n",
      "Epoch 2 Avg. Loss: 0.6062566637992859\n",
      "Running val phase...\n",
      "Calculating batch 0 / 59\n",
      "Calculating batch 10 / 59\n",
      "Calculating batch 20 / 59\n",
      "Calculating batch 30 / 59\n",
      "Calculating batch 40 / 59\n",
      "Calculating batch 50 / 59\n",
      "Epoch 2 Avg. MAA: 0.5796675647158962, Best MAA: 0.5789808541516696\n",
      "\n",
      "\n",
      "List of avg. loss across epochs: \n",
      "[0.6175587773323059, 0.6089193820953369, 0.6062566637992859]\n",
      "List of losses across batches: \n",
      "[0.6824159026145935, 0.6751169562339783, 0.7061638236045837, 0.6793316602706909, 0.6641712784767151, 0.7001317739486694, 0.6888464689254761, 0.6720569729804993, 0.7123551964759827, 0.6723183393478394, 0.6698805093765259, 0.6754809617996216, 0.6783734560012817, 0.661333441734314, 0.6776391863822937, 0.6939587593078613, 0.677532970905304, 0.6771491765975952, 0.6859610676765442, 0.6882891654968262, 0.6505954265594482, 0.678777813911438, 0.6779786348342896, 0.6497977375984192, 0.6697885394096375, 0.663873553276062, 0.6899545192718506, 0.6830138564109802, 0.6594246625900269, 0.6645684838294983, 0.6518304347991943, 0.646695077419281, 0.6540504693984985, 0.6736801862716675, 0.6179763078689575, 0.6519169211387634, 0.6079538464546204, 0.6482172608375549, 0.6414148807525635, 0.6197019815444946, 0.633359968662262, 0.6111510992050171, 0.6670140027999878, 0.6192809343338013, 0.639062225818634, 0.6095882654190063, 0.6080623865127563, 0.6120331287384033, 0.6184268593788147, 0.6443620920181274, 0.6189312934875488, 0.6732085347175598, 0.6008864045143127, 0.642183244228363, 0.6351319551467896, 0.6253148317337036, 0.6189336180686951, 0.6564300060272217, 0.6292741894721985, 0.6124486327171326, 0.6033483743667603, 0.6510785818099976, 0.6314892768859863, 0.5905808210372925, 0.599449872970581, 0.6069957613945007, 0.6443109512329102, 0.6373813152313232, 0.6399974226951599, 0.6264058351516724, 0.6151474714279175, 0.6321414113044739, 0.6170382499694824, 0.6364030241966248, 0.6364394426345825, 0.6343831419944763, 0.6287859678268433, 0.6343067288398743, 0.5895459055900574, 0.607291579246521, 0.6469062566757202, 0.5961159467697144, 0.6115805506706238, 0.605021059513092, 0.6010627150535583, 0.610235869884491, 0.6093770265579224, 0.6437548398971558, 0.6075002551078796, 0.6409573554992676, 0.6049781441688538, 0.5883544087409973, 0.6153831481933594, 0.628944993019104, 0.6043374538421631, 0.6057736873626709, 0.6331594586372375, 0.6025987267494202, 0.5921719670295715, 0.6269828081130981, 0.6535855531692505, 0.6364398002624512, 0.6304782032966614, 0.6599568724632263, 0.6086207628250122, 0.6146258115768433, 0.606948733329773, 0.5962553024291992, 0.6078490018844604, 0.6538000106811523, 0.6077837347984314, 0.6092669367790222, 0.589789092540741, 0.5890281200408936, 0.5839816331863403, 0.620841383934021, 0.6124545931816101, 0.619799017906189, 0.6339693069458008, 0.6034788489341736, 0.6292722821235657, 0.6581191420555115, 0.6316133141517639, 0.623846173286438, 0.611429750919342, 0.5934165120124817, 0.5992158651351929, 0.6096466779708862, 0.5855087041854858, 0.6265538930892944, 0.5849030613899231, 0.5758976340293884, 0.6837936043739319, 0.5809696912765503, 0.6437143683433533, 0.61661297082901, 0.5800657868385315, 0.6198195219039917, 0.5991535782814026, 0.6277269721031189, 0.5864458680152893, 0.6543582081794739, 0.6508215665817261, 0.6282905340194702, 0.6202594041824341, 0.646300196647644, 0.6036469340324402, 0.6225100159645081, 0.6074740290641785, 0.6457580327987671, 0.574780285358429, 0.5988824367523193, 0.5914316773414612, 0.5671902894973755, 0.6553984880447388, 0.6361952424049377, 0.6334578990936279, 0.5931407809257507, 0.5992477536201477, 0.6227449178695679, 0.6188887357711792, 0.6260309219360352, 0.5902955532073975, 0.5902788043022156, 0.660396933555603, 0.5798956751823425, 0.6276435852050781, 0.6237706542015076, 0.6100009083747864, 0.6400972604751587, 0.6155335903167725, 0.6013805866241455, 0.6169090867042542, 0.6061723828315735, 0.6240243315696716, 0.5959360003471375, 0.637698769569397, 0.5979282855987549, 0.6178919672966003, 0.6019390821456909, 0.5517512559890747, 0.5891612768173218, 0.6320787072181702, 0.6312957406044006, 0.5820213556289673, 0.5704134106636047, 0.642318069934845, 0.590421199798584, 0.6053177118301392, 0.6034968495368958, 0.6063425540924072, 0.6392318606376648, 0.5954104661941528, 0.5787585377693176, 0.604282021522522, 0.6078997850418091, 0.605014443397522, 0.5677696466445923, 0.5972999334335327, 0.6249511241912842, 0.6167085766792297, 0.6263958215713501, 0.6170048117637634, 0.6197032928466797, 0.60993891954422, 0.6103876233100891, 0.5965083241462708, 0.6103802919387817, 0.61770099401474, 0.6376579999923706, 0.6314657926559448, 0.6202078461647034, 0.6442619562149048, 0.5863559246063232, 0.6447647213935852, 0.6006584167480469, 0.59583580493927, 0.5644239187240601, 0.6089030504226685, 0.6387964487075806, 0.641158938407898, 0.6447847485542297, 0.6264014840126038, 0.6317808032035828, 0.6102567315101624, 0.593356192111969, 0.582194983959198, 0.6167815923690796, 0.5767029523849487, 0.5985454320907593, 0.6167698502540588, 0.5937084555625916, 0.5958834290504456, 0.5713983774185181, 0.6234493851661682, 0.5736883878707886, 0.5894160270690918, 0.5798712968826294, 0.6099288463592529, 0.5514371395111084, 0.6113336086273193, 0.6264718770980835, 0.6321620345115662, 0.6167793273925781, 0.5582866072654724, 0.5849574208259583, 0.62211012840271, 0.6158171892166138, 0.6127852201461792, 0.6226998567581177, 0.5994634628295898, 0.6033461689949036, 0.5913525223731995, 0.6048939824104309, 0.5952326059341431, 0.6046600341796875, 0.6301994323730469, 0.6140771508216858, 0.6274145841598511, 0.6079229116439819, 0.6125442385673523, 0.6194537878036499, 0.6533371806144714, 0.5802993178367615, 0.6201232075691223, 0.6037010550498962, 0.6120962500572205, 0.5762292146682739, 0.5661247968673706, 0.6208682060241699, 0.6420161724090576, 0.6430455446243286, 0.5920262336730957, 0.5444022417068481, 0.6257578730583191, 0.608025848865509, 0.5620556473731995, 0.5722910165786743, 0.5986550450325012, 0.5713776350021362, 0.5691455602645874, 0.5609555840492249, 0.6366971731185913, 0.5667964220046997, 0.6214390993118286, 0.6773377656936646, 0.5653881430625916, 0.614174485206604, 0.5914071202278137, 0.622749388217926, 0.5686367750167847, 0.6249111890792847, 0.591540515422821, 0.6024432182312012, 0.6123040318489075, 0.6313198208808899, 0.6049942374229431, 0.5714293718338013, 0.6272643208503723, 0.5984032154083252, 0.624907910823822, 0.5862795114517212, 0.6277207136154175, 0.6140496134757996, 0.6309501528739929, 0.5865044593811035, 0.5992549657821655, 0.6093127131462097, 0.615705668926239, 0.6334375143051147, 0.626655101776123, 0.6152854561805725, 0.6085731387138367, 0.6111153364181519, 0.627194344997406, 0.6368064880371094, 0.5894210934638977, 0.6387867331504822, 0.6164223551750183, 0.6044407486915588, 0.6507402658462524, 0.6321355104446411, 0.5730636119842529, 0.6159733533859253, 0.5849499106407166, 0.6166728734970093, 0.575667142868042, 0.5960749387741089, 0.6638838648796082, 0.5909405946731567, 0.6565657258033752, 0.5837737917900085, 0.6483699679374695, 0.6538575887680054, 0.5973032116889954, 0.5637092590332031, 0.6191852688789368, 0.6267703175544739, 0.6135349869728088, 0.6191665530204773, 0.6019358038902283, 0.6453161239624023, 0.6291378140449524, 0.5829522609710693, 0.6031991839408875, 0.5824693441390991, 0.588180661201477, 0.6126419901847839, 0.5541073083877563, 0.5818343758583069, 0.6221818923950195, 0.5892906785011292, 0.6319854855537415, 0.5749184489250183, 0.6237404942512512, 0.5802325010299683, 0.5953857898712158, 0.5848762392997742, 0.584073007106781, 0.5903627872467041, 0.6420204043388367, 0.5950973629951477, 0.5647367238998413, 0.6361060738563538, 0.6178172826766968, 0.5741222500801086, 0.6311195492744446, 0.6226836442947388, 0.645228385925293, 0.6112169027328491, 0.6105425953865051, 0.5838712453842163, 0.6323176622390747, 0.6287224292755127, 0.6410208940505981, 0.6140999794006348, 0.6037834882736206, 0.642926812171936, 0.5925005674362183, 0.5909855365753174, 0.6227439641952515, 0.5881420373916626, 0.6436799168586731, 0.6366580724716187, 0.6017245054244995, 0.5978288650512695, 0.6223162412643433, 0.624873161315918, 0.5826994776725769, 0.5899388790130615, 0.65107262134552, 0.6368027925491333, 0.6237969398498535, 0.6254533529281616, 0.6074476838111877, 0.5760082602500916, 0.5746041536331177, 0.64287930727005, 0.6066197752952576, 0.6245838403701782, 0.6259403824806213, 0.5914482474327087, 0.6330159902572632, 0.5934246778488159, 0.6369251012802124, 0.6087827682495117, 0.6060212850570679, 0.6188710927963257, 0.5791409015655518, 0.6014041900634766, 0.5880004167556763, 0.5545202493667603, 0.58537358045578, 0.6288509368896484, 0.6474379301071167, 0.5879375338554382, 0.6178500056266785, 0.6197283267974854, 0.5915291905403137, 0.6652118563652039, 0.6070870757102966, 0.6313742399215698, 0.6303547620773315, 0.5891516804695129, 0.6220563650131226, 0.6277557611465454, 0.6156595349311829, 0.6556459665298462, 0.5882294774055481, 0.6470057964324951, 0.6290422677993774, 0.6077952980995178, 0.584493100643158, 0.6102571487426758, 0.6338143348693848, 0.6016479730606079, 0.6364206075668335, 0.5843478441238403, 0.6035910248756409, 0.5681494474411011, 0.6214634776115417, 0.5745445489883423, 0.5718626379966736, 0.5756295323371887, 0.5821494460105896, 0.561633288860321, 0.61479651927948, 0.6310338377952576, 0.5892561078071594, 0.6232284307479858, 0.6041212677955627, 0.6111205816268921, 0.6051937341690063, 0.62709641456604, 0.6207183003425598, 0.6132308840751648, 0.6098161935806274, 0.5873538851737976, 0.5875855088233948, 0.6347182989120483, 0.5971574783325195, 0.6212659478187561, 0.6301057934761047, 0.6352974772453308, 0.5883913636207581, 0.6214766502380371, 0.623305082321167, 0.6149071455001831, 0.5652457475662231, 0.6911231875419617, 0.6384081840515137, 0.6339454054832458, 0.6043237447738647, 0.6269909739494324, 0.6109920740127563, 0.5972177982330322, 0.6144266724586487, 0.6271923780441284, 0.6456603407859802, 0.6352452039718628, 0.6185077428817749, 0.61837238073349, 0.6203490495681763, 0.6313601732254028, 0.6028919816017151, 0.598919153213501, 0.6191380023956299, 0.5704044103622437, 0.5859242677688599, 0.6203358769416809, 0.6044946312904358, 0.602725625038147, 0.6213192939758301, 0.6325353384017944, 0.606852114200592, 0.6064469814300537, 0.5865438580513, 0.620746374130249, 0.5986934900283813, 0.6254405379295349, 0.6374288201332092, 0.6276615858078003, 0.6463100910186768, 0.6004568338394165, 0.6047548651695251, 0.6530494093894958, 0.5951929092407227, 0.6061131358146667, 0.6454402208328247, 0.5814281105995178, 0.6358150243759155, 0.6446915864944458, 0.5730070471763611, 0.6352514028549194, 0.5739910006523132, 0.6147027611732483, 0.6390839219093323, 0.6136871576309204, 0.6242610216140747, 0.6056981086730957, 0.6108444333076477, 0.5783525705337524, 0.6390923261642456, 0.6187252998352051, 0.6622163653373718, 0.6049370765686035, 0.5761967897415161, 0.5728170275688171, 0.597710371017456, 0.6179073452949524, 0.5977724194526672, 0.6243255734443665, 0.6139041781425476, 0.5773055553436279, 0.7138517498970032, 0.5850577354431152, 0.596579909324646, 0.6305907964706421, 0.5984640717506409, 0.6207578778266907, 0.6015182137489319, 0.639419436454773, 0.6035482883453369, 0.6338874697685242, 0.6187602877616882, 0.6091273427009583, 0.5997687578201294, 0.6019060611724854, 0.5891503691673279, 0.59002685546875, 0.5924857258796692, 0.5942519903182983, 0.6166364550590515, 0.6265221238136292, 0.6396881341934204, 0.5689684152603149, 0.5614572167396545, 0.614498496055603, 0.6034329533576965, 0.6018597483634949, 0.6320055723190308, 0.5705909132957458, 0.5864511132240295, 0.5989827513694763, 0.5800958871841431, 0.5817118883132935, 0.6190257668495178, 0.6142879128456116, 0.5903543829917908, 0.5731940269470215, 0.5999925136566162, 0.6246529817581177, 0.5861788988113403, 0.597681999206543, 0.6246402859687805, 0.5922877788543701, 0.6256254315376282, 0.553408145904541, 0.6117411255836487, 0.5898367166519165, 0.6136974096298218, 0.5929611325263977, 0.6517238020896912, 0.5946405529975891, 0.6260679364204407, 0.6016252636909485, 0.6190120577812195, 0.6140059232711792, 0.6247240304946899, 0.6020929217338562, 0.5877900719642639, 0.5862077474594116, 0.6476625204086304, 0.5660898685455322, 0.5869719982147217, 0.630222737789154, 0.5588667988777161, 0.6061859726905823, 0.5719320774078369, 0.5988386869430542, 0.5512610077857971, 0.5888680219650269, 0.6103419661521912, 0.671291172504425, 0.596983790397644, 0.5796076059341431, 0.565134584903717, 0.6188969016075134, 0.5861314535140991, 0.614850640296936, 0.6295773983001709, 0.5905064344406128, 0.6543105840682983, 0.6345446705818176, 0.5844531059265137, 0.617497444152832, 0.6502641439437866, 0.6171820759773254, 0.5942894220352173, 0.6016595363616943, 0.5582606792449951, 0.6034899950027466, 0.6667328476905823, 0.6387226581573486, 0.5795974731445312, 0.5833520889282227, 0.6043781042098999, 0.6032604575157166, 0.6257911920547485, 0.6113244891166687, 0.6332723498344421, 0.6262692809104919, 0.6193174123764038, 0.6087179183959961, 0.6631739139556885, 0.6266679763793945, 0.5935258269309998, 0.6125668287277222, 0.6438478231430054, 0.6089751720428467, 0.5959455370903015, 0.6333333849906921, 0.6237985491752625, 0.6181930303573608, 0.6308051943778992, 0.6105991005897522, 0.5760538578033447, 0.6118358969688416, 0.5917471647262573, 0.5758661031723022, 0.6554712057113647, 0.5920661687850952, 0.5985090732574463, 0.6160723567008972, 0.5939285159111023, 0.6295958757400513, 0.609294056892395, 0.5669199228286743, 0.6229314804077148, 0.5911952257156372, 0.5776624083518982, 0.587916374206543, 0.5678468346595764, 0.6113373041152954, 0.560827374458313, 0.6088431477546692, 0.6302794814109802, 0.6780520677566528, 0.6106147766113281, 0.5938698053359985, 0.5743380188941956, 0.5934519171714783, 0.6027191281318665, 0.6561751961708069, 0.5592395067214966, 0.6260024905204773, 0.541424036026001, 0.5734914541244507, 0.6476700901985168, 0.6711229085922241, 0.5643621683120728, 0.593136191368103, 0.6453102827072144, 0.6377856135368347, 0.6108600497245789, 0.6316639184951782, 0.5830457210540771, 0.6131341457366943, 0.6159549951553345, 0.6090797185897827, 0.5915220975875854, 0.582880973815918, 0.5969755053520203, 0.5567978024482727, 0.5742471218109131, 0.6042969822883606, 0.5477176904678345, 0.6070529222488403, 0.6359490752220154, 0.6496114730834961, 0.6214271783828735, 0.5968719720840454, 0.6296623349189758, 0.6260015368461609, 0.6138582229614258, 0.6099380850791931, 0.6164454221725464, 0.6103380918502808, 0.6065269708633423, 0.6073503494262695, 0.603004515171051, 0.5896809101104736, 0.602450966835022, 0.6158278584480286, 0.54796302318573, 0.6191882491111755, 0.6396952271461487, 0.6132479906082153, 0.6055198907852173, 0.5716872215270996, 0.6048415899276733, 0.6213529706001282, 0.6080127954483032, 0.5963274240493774, 0.5721839070320129, 0.635746955871582, 0.6128285527229309, 0.6473072171211243, 0.5968772768974304, 0.6188071370124817, 0.6257696151733398, 0.646108090877533, 0.6001355648040771, 0.6045756340026855, 0.6540480852127075, 0.5943462252616882, 0.6326828598976135, 0.5700523257255554, 0.6062197685241699, 0.5858825445175171, 0.6093224287033081, 0.559409499168396, 0.5858711004257202, 0.6268256902694702, 0.6130400896072388, 0.6141161918640137, 0.6075510382652283, 0.5912951231002808, 0.5971795320510864, 0.5935105085372925, 0.6198949217796326, 0.5917856097221375, 0.6064242124557495, 0.6231709122657776, 0.6341029405593872, 0.5764867067337036, 0.6050771474838257, 0.6175517439842224, 0.6333335638046265, 0.5742059946060181, 0.6070824861526489, 0.6042052507400513, 0.6336950659751892, 0.6112498044967651, 0.5823808312416077, 0.6383137702941895, 0.6419165730476379, 0.6299070119857788, 0.6776880025863647, 0.618983805179596, 0.6010182499885559, 0.6046538352966309, 0.6460431814193726, 0.5837916135787964, 0.6246204376220703, 0.5834872722625732, 0.6588515043258667, 0.6490892171859741, 0.6169005036354065, 0.6106722950935364, 0.6271454095840454, 0.5465863347053528, 0.6382615566253662, 0.5803205370903015, 0.5852066278457642, 0.6065865755081177, 0.5946801900863647, 0.5920775532722473, 0.6211403608322144, 0.574169933795929, 0.6276594400405884, 0.6291380524635315, 0.581798255443573, 0.6331401467323303, 0.6277846693992615, 0.6131874322891235, 0.6546186208724976, 0.565953254699707, 0.5849552154541016, 0.6145866513252258, 0.6078675985336304, 0.6199965476989746, 0.5905989408493042, 0.6421653032302856, 0.6034622192382812, 0.5802015066146851, 0.6331788301467896, 0.598896861076355, 0.6451278924942017, 0.6614632606506348, 0.6113528609275818, 0.6390174627304077, 0.5975265502929688, 0.6371755003929138, 0.5887116193771362, 0.6012887954711914, 0.6016389727592468, 0.6015127897262573, 0.5899465084075928, 0.6408097743988037, 0.6275531649589539, 0.6033695340156555, 0.60694420337677, 0.6223081350326538, 0.6253536939620972, 0.5959629416465759, 0.5891774892807007, 0.6360389590263367, 0.6450561285018921, 0.5682443380355835, 0.6173574924468994, 0.6508831977844238, 0.5916879177093506, 0.607426106929779, 0.5994669198989868, 0.6108186841011047, 0.5971971750259399, 0.6162998080253601, 0.5917515158653259, 0.5806424617767334, 0.5860462188720703, 0.6007218360900879, 0.6041208505630493, 0.564824640750885, 0.6400338411331177, 0.6450828313827515, 0.583940863609314, 0.6391987204551697, 0.5738295912742615, 0.6055279970169067, 0.5244401693344116, 0.6267088651657104, 0.6174275279045105, 0.6015411019325256, 0.5822945833206177, 0.624360203742981, 0.62332683801651, 0.5881283283233643, 0.6001976728439331, 0.6308440566062927, 0.6332873702049255, 0.6149412393569946, 0.6140024662017822, 0.6279841661453247, 0.5895881652832031, 0.6376258134841919, 0.6107702255249023, 0.5997294783592224, 0.5901726484298706, 0.6198654770851135, 0.5947174429893494, 0.6210460066795349, 0.6008943319320679, 0.5969641208648682, 0.5669666528701782, 0.5944456458091736, 0.5822027921676636, 0.5638378262519836, 0.6259157657623291, 0.658571183681488, 0.6210392713546753, 0.5890352725982666, 0.6477916836738586, 0.602936863899231, 0.593993067741394, 0.5742484927177429, 0.6303730607032776, 0.6134601831436157, 0.5952883362770081, 0.6126011610031128, 0.6457991600036621, 0.5832844972610474, 0.5921818017959595, 0.6106804013252258, 0.6047794818878174, 0.5838664174079895, 0.561447262763977, 0.5864342451095581, 0.5912126898765564, 0.6135026812553406, 0.6216591596603394, 0.6484575271606445, 0.6216164231300354, 0.6069015860557556, 0.6965426206588745, 0.6018260717391968, 0.6209751963615417, 0.600134551525116, 0.6189258098602295, 0.6207781434059143, 0.6318106651306152, 0.6078162789344788, 0.6057507395744324, 0.653344988822937, 0.6489881277084351, 0.580776035785675, 0.6059935688972473, 0.6048460602760315, 0.6190227270126343, 0.6040724515914917, 0.5979872941970825, 0.5899714827537537, 0.56731116771698, 0.5739346742630005, 0.5932033658027649, 0.6640259623527527, 0.596008837223053, 0.6181963682174683, 0.5735964775085449, 0.5615826845169067, 0.6012316346168518, 0.5795372724533081, 0.6082936525344849, 0.6110583543777466, 0.6569392681121826, 0.5709717273712158, 0.5919941663742065, 0.6110551357269287, 0.5932523012161255, 0.6050523519515991, 0.6014028787612915, 0.5727118253707886, 0.5951181054115295, 0.6166064143180847, 0.6086798906326294, 0.6852419376373291, 0.5809511542320251, 0.5933791399002075, 0.6136888265609741, 0.6787084937095642, 0.6314557790756226, 0.6543042063713074, 0.6124247312545776, 0.6111620664596558, 0.6078444123268127, 0.578536868095398, 0.6367671489715576, 0.5973811745643616, 0.5927783250808716, 0.5897016525268555, 0.6385713815689087, 0.5912820100784302, 0.5940364003181458, 0.6188492178916931, 0.619055449962616, 0.5967792272567749, 0.6139756441116333, 0.6391028165817261, 0.5798835158348083, 0.6299241781234741, 0.6229647994041443, 0.6045218706130981, 0.6012344360351562, 0.5690085291862488, 0.597281813621521, 0.5184804201126099, 0.6216809153556824, 0.6100393533706665, 0.5978707671165466, 0.5576297044754028, 0.623348593711853, 0.5839674472808838, 0.585678219795227, 0.594450831413269, 0.5701900720596313, 0.5863897800445557, 0.6604782938957214, 0.6128426790237427, 0.6098315715789795, 0.6075789928436279, 0.6228300333023071, 0.5842813849449158, 0.5298928618431091, 0.6278418302536011, 0.6167993545532227, 0.6484246253967285, 0.5992225408554077, 0.5883764028549194, 0.565850555896759, 0.6101284623146057, 0.6137195825576782, 0.5779005885124207, 0.6153588891029358, 0.6050480604171753, 0.5571876168251038, 0.6035645604133606, 0.609580934047699, 0.5612227916717529, 0.5990204811096191, 0.6189996004104614, 0.5904663801193237, 0.6317428946495056, 0.6092462539672852, 0.5968825221061707, 0.6074146032333374, 0.6022953987121582, 0.614965558052063, 0.5895305871963501, 0.568924069404602, 0.5816558003425598, 0.6248912811279297, 0.6059252023696899, 0.5937464833259583, 0.5811995267868042, 0.5692845582962036, 0.5770198106765747, 0.6318777799606323, 0.6286066770553589, 0.6222596764564514, 0.5500351786613464, 0.6094744801521301, 0.6031228303909302, 0.614298939704895, 0.6059955358505249, 0.6549410223960876, 0.5989677309989929, 0.6930041909217834, 0.5897501707077026, 0.5927112698554993, 0.5947794318199158, 0.6145557165145874, 0.5990424156188965, 0.577112078666687, 0.5782999992370605, 0.6066133975982666, 0.6281492114067078, 0.5956211686134338, 0.6201289296150208, 0.6336942315101624, 0.6000998616218567, 0.5981458425521851, 0.6136273145675659, 0.6043365597724915, 0.6297522783279419, 0.6579654216766357, 0.5992275476455688, 0.564285397529602, 0.6180370450019836, 0.5633876919746399, 0.6481913924217224, 0.6446883678436279, 0.5834931135177612, 0.6523887515068054, 0.5882707834243774, 0.5755756497383118, 0.6285446286201477, 0.5904051065444946, 0.595356822013855, 0.600753903388977, 0.6107594966888428, 0.5937261581420898, 0.6247765421867371, 0.6017806529998779, 0.5799402594566345, 0.62328040599823, 0.6149056553840637, 0.6217299699783325, 0.6132420301437378, 0.5600014925003052, 0.6392955183982849, 0.6170213222503662, 0.6080461740493774, 0.6245818138122559, 0.60033118724823, 0.6344887018203735, 0.6174562573432922, 0.5983595848083496, 0.599420428276062, 0.6128815412521362, 0.6060539484024048, 0.5702100396156311, 0.5892051458358765, 0.6285570859909058, 0.5921720266342163, 0.6509660482406616, 0.6295936703681946, 0.6234911680221558, 0.6476441621780396, 0.5956016778945923, 0.5805882215499878, 0.5955711007118225, 0.6043835282325745, 0.6167224049568176, 0.5734174847602844, 0.5791810154914856, 0.6502603888511658, 0.5847955942153931, 0.6053308248519897, 0.5811392664909363, 0.6167727112770081, 0.5938316583633423, 0.5822473764419556, 0.6450055837631226, 0.5978771448135376, 0.604889452457428, 0.5852185487747192, 0.58353590965271, 0.6023174524307251, 0.615190863609314, 0.6284619569778442, 0.6202673316001892, 0.6262476444244385, 0.6176042556762695, 0.5870882272720337, 0.6117743253707886, 0.5945008993148804, 0.6374117732048035, 0.5819246768951416, 0.6319201588630676, 0.5725346803665161, 0.6425808668136597, 0.5980888605117798, 0.6254563927650452, 0.6086778044700623, 0.6425060629844666, 0.5988708734512329, 0.6082384586334229, 0.6432621479034424, 0.658877968788147, 0.6054835319519043, 0.6098605394363403, 0.6379107236862183, 0.6194816827774048, 0.5671643018722534, 0.6158576011657715, 0.5642606019973755, 0.6095606088638306, 0.5841361284255981, 0.5933636426925659, 0.6053592562675476, 0.6169141530990601, 0.6155563592910767, 0.5913563370704651, 0.5945158004760742, 0.6030402779579163, 0.5899551510810852, 0.6113665699958801, 0.601606011390686, 0.586364209651947, 0.621458113193512, 0.6107953190803528, 0.6167452931404114, 0.5956587791442871, 0.5445505380630493, 0.6252738237380981, 0.5856408476829529, 0.6072925925254822, 0.6200555562973022, 0.5353226661682129, 0.6290451288223267, 0.6205161809921265, 0.6232081651687622, 0.6186652183532715, 0.5543771386146545, 0.5886470079421997, 0.6216092705726624, 0.593899130821228, 0.6007736921310425, 0.6115580797195435, 0.6156784892082214, 0.5816893577575684, 0.6065085530281067, 0.622726559638977, 0.6202749013900757, 0.607119619846344, 0.635603666305542, 0.6087762713432312, 0.6274617910385132, 0.6115643978118896, 0.6225742697715759, 0.5920051336288452, 0.5882611870765686, 0.5810210108757019, 0.6225557923316956, 0.5713094472885132, 0.6049957275390625, 0.5726011395454407, 0.6235899329185486, 0.6232701539993286, 0.5816798806190491, 0.5687144994735718, 0.6362771987915039, 0.561451256275177]\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# # import multiprocessing as python_multiprocessing\n",
    "# # import torch.multiprocessing as multiprocessing\n",
    "# # from . import IterableDataset, Sampler, SequentialSampler, RandomSampler, BatchSampler\n",
    "# from . import _utils\n",
    "# from torch._utils import ExceptionWrapper\n",
    "# import threading\n",
    "# import itertools\n",
    "# from torch._six import queue, string_classesimport torch.utils.data.dataloader\n",
    "\n",
    "### TRAINING BLOC:\n",
    "model = BiLSTMEncoderDecoder(encode_size=67, decode_size=29).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 2.5e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "\n",
    "num_epochs = 3\n",
    "best_maa = 0\n",
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "# default_collate = _utils.collate.default_collate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    for phase in ['train', 'val']:\n",
    "        print(\"Running {} phase...\".format(phase))\n",
    "        total_maa = []\n",
    "        total_loss = []\n",
    "        dataloader = DataLoader(datasets[phase], batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=default_collate)\n",
    "        for i, (X_encode, X_decode, y) in enumerate(dataloader):\n",
    "            \n",
    "            # just testing on small sizes for now\n",
    "            if i > 400:\n",
    "                break\n",
    "                \n",
    "            if i % 10 == 0:\n",
    "                print(\"Calculating batch {} / {}\".format(i, len(dataloader)))\n",
    "            X_encode, X_decode, y = X_encode.to(device), X_decode.to(device), y.to(device)\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                scores = model(X_encode, X_decode).flatten(start_dim=0, end_dim=1)\n",
    "                labels = y.flatten(start_dim=0, end_dim=1).squeeze()\n",
    "                loss = loss_fn(scores, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.append(loss)\n",
    "                batch_losses.append(loss.item())\n",
    "            else:\n",
    "                model.eval()\n",
    "                scores = model(X_encode, X_decode).flatten(start_dim=0, end_dim=1)\n",
    "                labels = y.flatten(start_dim=0, end_dim=1).squeeze()\n",
    "                y_pred = torch.argmax(scores, dim=1)\n",
    "                maa = evaluate_batch_mean_average_accuracy(labels, y_pred)\n",
    "                total_maa.append(maa)\n",
    "        if phase == 'train':\n",
    "            epoch_loss = sum(total_loss) / len(total_loss)\n",
    "            print(\"Epoch {} Avg. Loss: {}\".format(epoch, epoch_loss))\n",
    "            epoch_losses.append(epoch_loss.item())\n",
    "        else:\n",
    "            epoch_maa = sum(total_maa) / len(total_maa)\n",
    "            print(\"Epoch {} Avg. MAA: {}, Best MAA: {}\".format(epoch, epoch_maa, best_maa))\n",
    "            if epoch_maa > best_maa:\n",
    "                torch.save(model.state_dict(), MODEL_PATH)\n",
    "                best_maa = epoch_maa\n",
    "    print()\n",
    "print()\n",
    "print(\"List of avg. loss across epochs: \")\n",
    "print(epoch_losses)\n",
    "save_list_to_csv(epoch_losses, EPOCH_LOSSES_SAVEPATH)\n",
    "print(\"List of losses across batches: \")\n",
    "print(batch_losses)\n",
    "save_list_to_csv(batch_losses, BATCH_LOSSES_SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average batch MAA over test set: 0.40402910915841944\n"
     ]
    }
   ],
   "source": [
    "# TESTING BLOCK\n",
    "test_model = BiLSTMEncoderDecoder(encode_size=67, decode_size=29).to(device)\n",
    "test_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataloader = DataLoader(datasets[\"test\"], batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=default_collate)\n",
    "    total_maa = []\n",
    "    for i, (X_encode, X_decode, labels) in enumerate(dataloader):\n",
    "#         if i > 40:\n",
    "#             break\n",
    "        X_encode, X_decode, y = X_encode.to(device), X_decode.to(device), y.to(device)\n",
    "        labels = y.flatten(start_dim=0, end_dim=1).squeeze()\n",
    "        scores = test_model(X_encode, X_decode).flatten(start_dim=0, end_dim=1)\n",
    "        y_pred = torch.argmax(scores, dim=1)\n",
    "        maa = evaluate_batch_mean_average_accuracy(labels, y_pred)\n",
    "        total_maa.append(maa)\n",
    "    print(\"Average batch MAA over test set: {}\".format(sum(total_maa) / len(total_maa)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
